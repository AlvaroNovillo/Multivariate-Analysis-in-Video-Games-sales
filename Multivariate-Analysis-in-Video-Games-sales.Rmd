---
output: 
  stevetemplates::article:
    fig_caption: true
bibliography: master.bib
biblio-style: apsr
title: "Multivariate Analysis in Video Games sales"
thanks: "Replication files are available on the author's Github account (https://github.com/AlvaroNovillo). **Current version**: `r format(Sys.time(), '%B %d, %Y')`; **Corresponding author**: alvanovi@ucm.es."
author:
- name: √Ålvaro Novillo
  affiliation: Universidad Carlos III
- name: Paolo Salvatore Lodato Olano
  affiliation: Universidad Carlos III
abstract: "In this article, we perform several dimensionality reduction techniques and clustering algorithms on a video game sales dataset available on Kaggle (https://www.kaggle.com/datasets/gregorut/videogamesales/data). Specifically, we use Principal Component Analysis (PCA) and Multidimensional Scaling (MDS) to reduce the dimensionality of the dataset. The article discusses the advantages and limitations of each technique and provides insights into the video game market based on the analysis."
keywords: "PCA, Videogames, Sales"
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
# spacing: double
endnote: no
---

```{r setup, include=FALSE}
library(kableExtra)
library(ggplot2)
options(kableExtra.latex.load_packages = TRUE)
knitr::opts_chunk$set(cache=TRUE,
                      message=FALSE, warning=FALSE,
                      fig.path='figs/',
                      cache.path = '_cache/',
                      fig.process = function(x) {
                      x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
                      if (file.rename(x, x2)) x2 else x
                      })
```

## About the dataset

The dataset under consideration contains information on video games with sales greater than 100,000 copies between 1980 and 2016. The dataset includes 11,493 unique game sales, detailing the name, year of release, genre, platform, and sales figures across numerous regions.

The dataset contains the following fields:

-   **Rank** - Ranked by overall sales
-   **Name** - Name of each videogame
-   **Platform** - The games platform
-   **Year** - Year of Release
-   **Genre** - Genre of Game
-   **Publisher** - Publisher of Game
-   **NA_Sales** - Sales in NA (per Million)
-   **EU_Sales** - Sales in EU (per Million)
-   **JP_Sales** - Sales in JP (per Million)
-   **Other_Sales** - Sales in ROW[^1] (per Million)
-   **Global_Sales** - Total worldwide sales (per Million)

[^1]: Net Sales (ROW) means the gross amount billed or invoiced on sales by Company and its Affiliates and Sublicensees of Licensed Products, less the following: (a) customary trade, quantity, or cash discounts and commissions to non-affiliated brokers or agents to the extent actually allowed and taken; (b) amounts repaid or credited by reason of rejection or return; (c) to the extent separately stated on purchase orders, invoices, or other documents of sale, any taxes or other governmental charges levied on the production, sale, transportation, delivery, or use of a Licensed Product which is paid by or on behalf of Company; (d) outbound transportation costs prepaid or allowed and costs of insurance in transit; and (e) allowance for bad debt that is customary and reasonable for the industry and in accordance with generally accepted accounting principles. [@lawinsider]

## Data Preprocessing

The dataset contains 11 variables, including quantitative variables like sales figures across various regions (NA_Sales, EU_Sales, JP_Sales, Other_Sales, and Global_Sales), the release year, and the rank of the game based on overall sales. Additionally, it includes multi-state categorical variables like the genre, platform, and publisher of the game. To conform with the desired format, which requires at least two binary variables, we will filter out the video games of recent years and focus on titles that we are already acquainted with. Moreover, we will limit our research to two primary platforms, namely, Xbox One and PS4.

```{r preprocessing, echo=FALSE}
df = read.csv("vgsales.csv")

# Filter for games in 2015 and 2016, and on the PS4 or PC platform
filtered_df <- df[df$Year %in% c(2015, 2016) & df$Platform %in% c("PS4", "XOne"), ]
head(filtered_df) %>%
  kbl(booktabs = T, caption = "Top five videogames, according to the sales ranking, that we are going to work with") %>%
  kable_styling(latex_options = c("striped", "scale_down","hold_position"), position = "center")

```

In Table 1. the top five selling games for 2015 and 2016, in PS4 ans Xbox One are shown. As we can see, the first one, which is Call Of Duty: Black Ops 3 is among the top 50 best selling games of the dataset (in PS4).

Examining the distribution of the filtered games rank, as seen in Fig. 1, considering its skewness, it can be confirmed that the vast majority of games released during this time period did not have a significant impact on the industry. In actuality, the average ranking of games within our dataset stands at 9373.

```{r rank_distrib,echo= FALSE,fig.cap = "Distribution of the log-transformed Rank values. The red dashed line represents the median of the distribution"}
# Create a histogram
# Calculate the median of the log-transformed Rank
median_rank <- median(log(filtered_df$Rank))

# Create a histogram with median line and text
ggplot(data = filtered_df, aes(y = log(Rank))) +
  geom_histogram(fill = "skyblue", color = "black") +
  geom_hline(yintercept = median_rank, linetype = "dashed", color = "red") +
  geom_text(aes(x = 70, y = median_rank - 0.1 ,label = round(exp(median_rank), 2)),
            vjust = -1, hjust = -0.2, color = "red") +
  labs(y = "log(Rank)") +
  theme_minimal()
```

Figs. 2 and 3 allow us to explore the basic features of our dataset, informing us of the amount of games from each platform, and the amount of games of each genre. In our dataset, the mayority of the sold games are from PS4, and the most popular genre is Action, followed by Sports, Role-Playing and Shooter

```{r num_games, echo=FALSE, fig.cap= "Number of games of each platform inside the dataset"}
# Load the ggplot2 library if not already loaded
library(ggplot2)

# Create a barplot of the number of games per platform with custom colors
platform_counts <- table(filtered_df$Platform)
platform_counts_df <- data.frame(Platform = names(platform_counts), Count = as.vector(platform_counts))

# Define custom colors for XOne and PS4
custom_colors <- c("XOne" = "green", "PS4" = "blue")

ggplot(data = platform_counts_df, aes(x = reorder(Platform, -Count), y = Count, fill = Platform)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = custom_colors) +  # Apply custom colors
  labs(x = "Platform",
       y = "Number of Games") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r num_genres, echo=FALSE, fig.cap= "Barplot of the amount of games of each genre"}
# Create a barplot of the number of games per platform with custom colors
genre_counts <- table(filtered_df$Genre)
genre_counts_df <- data.frame(Genre = names(genre_counts), Count = as.vector(genre_counts))

ggplot(data = genre_counts_df, aes(x = reorder(Genre, -Count), y = Count, fill = Genre)) +
  geom_bar(stat = "identity") +
  labs(x = "Genre",
       y = "Number of Games") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

From the previous plot we can see the Rank variable is right skewed. For positive skewness we can apply a log transformation^[Since the variables referring to the sales in our dataset presents zeros, we have applied Box-Cox technique (See @box) to identify the appropriate transformation for our case (obtaining $\lambda \approx 0$), leading to the application of $log(x + \epsilon)$ transformation, being $\epsilon$ an arbitrary small constant.], which would help to normalize the distribution of the variables. Many statistical methods, including linear regression and analysis of variance, assume that the residuals are normally distributed. Normalizing our data would also be useful to help achieve zero mean and unit variance. In Fig. 4, we can visualize these transformations performed on the sales variables.

```{r box_transform, echo=FALSE, fig.cap= "Boxplots of the different transformations applyied to the sales data. From left to right, the original data, the log-transformed data and the log transformed and scaled data"}
par(mfrow = c(1, 3))
data <- filtered_df[sapply(df,is.numeric)][,2:4]

# Create boxplots
boxplot(data, main = "Original Data")
boxplot(log(data + 0.001) , main = "Log Transformed Data")
boxplot(scale(log(data + 0.001)), main = "Log & Scaled Data")

data <- as.data.frame(scale(log(data + 0.001)))

```

Since the ranking is solely determined by overall sales figures, it is worthwhile investigating whether the top-selling game in certain regions differs from that of others. Our expectation is that the best-selling games in Japan will differ from those sold in the West. To do such analysis, we will start by computing the *correlation matrix* of the sales in the different regions. 

```{r, echo=FALSE}

covariance <- cor(data)
round(covariance,3) %>%
  kbl(booktabs = T, caption = "Covariance matrix of sales") %>%
  kable_styling(latex_options = c("striped","hold_position"), position = "center")

```

The *correlation matrix* can be computed as follows: $\text{Cor}(X, Y) = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}$, where $\text{Cov}(X, Y)$ is the covariance between variables $X$ and $Y$, and $\sigma_X$ and $\sigma_Y$ are their respective standard deviations. The correlation matrix provides a comprehensive view of the linear relationships between variables in a dataset. It is useful for identifying patterns, understanding dependencies, and detecting multicollinearity in statistical analyses.

Doing so (Table 2), we can see that the correlation between the sales in Japan and the West is 0.394, with an even lower correlation of 0.299 with the North American market, as illustrated in Fig. 4. This raises compelling questions about the underlying factors that contribute to these correlations. It is clear that several key factors highlight the significant differences between the Oriental and Western video game industries, leading to this low correlation. 

First and foremost, the contrast in gaming preferences between regions plays a key role. As seen in the intercorrelation measurements, there is some correlation in the sales. In the West, specifically in North America, action and shooter games are incredibly popular. However, the Japanese market favours Role Playing Games (RPGs), which differs greatly from the Western market. As a result of these diverging gaming genres, differing sales patterns naturally occur, ultimately contributing to the observed low correlation.

The marketing and localization strategies utilised in the Japanese video game industry are of great significance. Numerous Japanese games are designed with a primary focus on the local market, giving rise to gameplay and cultural elements that may not resonate as strongly with Western or North American audiences. Therefore, these games may not be successful beyond their intended audience in the East, resulting in a larger sales gap and a weaker association with these regions.

On the contrary, the sales in North America and Europe are more correlated as these regions share similar western cultures and comparable marketing strategies. Conversely, Japan presents a more distinct market, with its citizens' tastes significantly differing from those of western cultures. Fig. 4 displays a correlation matrix using a heatmap to visualize this relationship.

It is also worth noting that given the nature of the variables, they all present a positive correlation. To ensure the low *intercorrelation* between our sales data, we have computed different correlation measures^[The computed metrics are the ones that have been sugested in class. See @aurea1], presented in Table 3. 


```{r, echo=FALSE}
intercorrelations <- function(X) {
  # Get the dimensions of the matrix X
  dim_X <- dim(X)
  n <- dim_X[1]
  p <- dim_X[2]
  
  # Compute the correlation matrix R
  R <- cor(X)
  
  # Initialize the output vector q
  q <- rep(0, 6)
  
  # Compute eigenvalues of R
  lambda <- eigen(R)$values
  
  # Compute diagonal elements of the inverse of R
  rjj <- diag(solve(R))
  
  # Calculate intercorrelations measures
  q[1] <- (1 - min(lambda) / max(lambda))^(p + 2)
  q[2] <- 1 - p / sum(1 / lambda)
  q[3] <- 1 - sqrt(det(R))
  q[4] <- (max(lambda) / p)^(3/2)
  q[5] <- (1 - min(lambda) / p)^5
  q[6] <- sum((1 - 1/rjj) / p)
  
  # Return the result vector q
  return(q)
}

q <- intercorrelations(data)
q <- t(as.data.frame(q))
colnames(q) <- c('q1','q2','q3','q4','q5','q6')
round(q,3) %>%
  kbl(booktabs = T, caption = "Correlation measurements of the sales in the different regions") %>%
  kable_styling(latex_options = c("striped","hold_position"), position = "center")
```

As noted earlier when examining the correlation between pairs of markets, the intercorrelation between the three markets is low because of the aforementioned socio-cultural factors.

```{r, echo=FALSE}
library(GGally)
library(gridExtra)
library(cowplot)
library(ggpubr)

plot.mpg <- ggpairs(data, 
                    lower = list(mapping = aes(color = filtered_df$Genre)),
                    diag = list(continuous = "barDiag")) +    
    scale_fill_manual(values=c("Shooter" = "#FF5733",      # Assigning colors to each category
                                "Sports" = "#3366FF", 
                                "Role-Playing" = "#FF33FF", 
                                "Action" = "#FFCC33", 
                                "Fighting" = "#33FF57", 
                                "Racing" = "#FF3366", 
                                "Adventure" = "#33FFFF",
                                "Platform" = "#9966FF",
                                "Misc" = "#999999",
                                "Simulation" = "#FF9933",
                                "Strategy" = "#33FFCC",
                                "Puzzle" = "#FF33CC")) +
    scale_colour_manual(values=c("Shooter" = "#FF5733",      # Assigning colors to each category
                                "Sports" = "#3366FF", 
                                "Role-Playing" = "#FF33FF", 
                                "Action" = "#FFCC33", 
                                "Fighting" = "#33FF57", 
                                "Racing" = "#FF3366", 
                                "Adventure" = "#33FFFF",
                                "Platform" = "#9966FF",
                                "Misc" = "#999999",
                                "Simulation" = "#FF9933",
                                "Strategy" = "#33FFCC",
                                "Puzzle" = "#FF33CC"))

# Save the ggpairs plot as a PDF
pdf("ggpairs_plot.pdf")
print(plot.mpg)
# Convert the saved ggpairs plot to a grob
saved_plot <- grid::grid.grab()


# Generate the legend separately
legend_plot <- ggplot(data, aes(x = 1, y = 1, color = filtered_df$Genre)) +
  geom_point(size = 5) +
  scale_color_manual(name = "Genre",  # Legend title
                     values = c("Shooter" = "#FF5733",      # Assigning colors to each category
                                "Sports" = "#3366FF", 
                                "Role-Playing" = "#FF33FF", 
                                "Action" = "#FFCC33", 
                                "Fighting" = "#33FF57", 
                                "Racing" = "#FF3366", 
                                "Adventure" = "#33FFFF",
                                "Platform" = "#9966FF",
                                "Misc" = "#999999",
                                "Simulation" = "#FF9933",
                                "Strategy" = "#33FFCC",
                                "Puzzle" = "#FF33CC"))  +
  theme_void() +
  theme(legend.position = "right")


# Extract the legend. Returns a gtable
leg <- get_legend(legend_plot)

# Convert to a ggplot and print
# Arrange the plots side by side
par(mfrow = c(1, 2))
plot = grid.arrange(saved_plot, as_ggplot(leg), ncol = 2,widths = c(0.8, 0.2))
```

```{r sales_pairs,echo=FALSE,fig.cap= "Correlation Plot of Video Game Sales in Different Regions"}
as_ggplot(plot)
```

To delve deeper into the differences in the market, Table 4 presents a comprehensive analysis of the percentage distribution of sales across the top three genres within diverse regions under investigation. It is evident from the table that the genre of Role-Playing Games (RPGs) enjoys significantly greater popularity in Japan as compared to North America and Europe. Strikingly, our research reveals that Action games emerge as the most prevalent genre in Japan, accounting for a substantial portion of the region's total sales, encompassing 35.28% of the market share.This results can also be found in Fig. 5, where we can see almost all Role-playing and Action games above the diagonal line (y = x), indicating the higher popularity of this genres in the Japanese market  

```{r top_genre,echo=FALSE}
library(dplyr)

# Group and summarize the data by genre for each region
genre_sales <- filtered_df %>%
  group_by(Genre) %>%
  summarise(
    Total_NA_Sales = sum(NA_Sales),
    Total_EU_Sales = sum(EU_Sales),
    Total_JP_Sales = sum(JP_Sales)
  ) %>%
  ungroup()

# Calculate the total sales for each region
total_sales <- genre_sales %>%
  summarise(
    Total_NA_Sales = sum(Total_NA_Sales),
    Total_EU_Sales = sum(Total_EU_Sales),
    Total_JP_Sales = sum(Total_JP_Sales)
  )

# Calculate the percentage of total sales for each genre in each region
genre_sales <- genre_sales %>%
  mutate(
    Percentage_NA_Sales = round((Total_NA_Sales / total_sales$Total_NA_Sales) * 100,2),
    Percentage_EU_Sales = round((Total_EU_Sales / total_sales$Total_EU_Sales) * 100,2),
    Percentage_JP_Sales = round((Total_JP_Sales / total_sales$Total_JP_Sales) * 100,2)
  )

# Select the top 3 genres for each region
top_genres <- genre_sales %>%
  mutate(
    Rank_NA = row_number(desc(Percentage_NA_Sales)),
    Rank_EU = row_number(desc(Percentage_EU_Sales)),
    Rank_JP = row_number(desc(Percentage_JP_Sales))
  ) %>%
  filter(Rank_NA <= 2 | Rank_EU <= 2 | Rank_JP <= 2) %>%
  select(Genre, Percentage_NA_Sales, Percentage_EU_Sales, Percentage_JP_Sales)



top_genres %>%
  kbl(booktabs = T, caption = "Percentage distribution of sales for the top three genres in different regions") %>%
  kable_styling(latex_options = c("striped", "scale_down","hold_position"), position = "center")
```

After scaling our data, we are now prepared to perform Principal Component Analysis (PCA). Scaling is a crucial preprocessing step as it ensures that each variable contributes equally to the analysis by standardizing their scales. PCA is particularly sensitive to the scale of variables, and standardizing them helps prevent variables with larger scales from dominating the analysis.

Principal Component Analysis (PCA) is a crucial step in our data analysis pipeline for several reasons. PCA allows us to effectively reduce the dimensionality of our dataset by transforming the original variables into a set of uncorrelated principal components, therefore addressing multicollinearity, capturing the essential information with fewer variables. This reduction is particularly valuable when dealing with high-dimensional datasets, enabling more manageable and interpretable analyses. Furthermore, PCA helps reveal underlying patterns and relationships within the data by highlighting the variables that contribute most to the observed variability. 

```{r, echo=FALSE}
data <- scale(log(filtered_df[sapply(filtered_df,is.numeric)][2:6] + 0.001))

# Perform PCA
pca_result <- prcomp(data, scale = TRUE)

# View the summary of the PCA
summary(pca_result)$importance  %>%
  kbl(booktabs = T, caption = "Summary of PCA") %>%
  kable_styling(latex_options = c("striped", "scale_down","hold_position"), position = "center")
```

As seen by the results, 0.97 of the total variability in the dataset has been explained by the three first components. The cumulative proportion of variance explained by each of the last two principal component is can be neglected. By using the package ${\tt factoextra}$ we can create a Scree plot which help decide on the number of components or factors to retain in the analysis. It displays the eigenvalues of the principal components or factors in descending order.

```{r, echo=FALSE}
library(factoextra)
fviz_eig(pca_result, addlabels = TRUE, ylim = c(0, 75))
```

```{r, echo=FALSE}
fviz_pca_var(pca_result, col.var = "black")
```

```{r, echo=FALSE}
fviz_pca_ind(pca_result,
             geom.ind = "point", # show points only (nbut not "text")
             col.ind = filtered_df$Platform, # color by groups
             palette = c("#00AFBB", "#FC4E07"),
             addEllipses = TRUE, # Concentration ellipses
             legend.title = "Groups"
             )
```
The ${\tt fviz\_pca\_biplot}$ function in R, part of the ${\tt factoextra}$ package, creates a biplot for Principal Component Analysis (PCA) results. A biplot simultaneously displays both the observations and variables in the same plot, allowing for a quick visual assessment of relationships between them. Due to the large size of our dataset we will display only the top 30 most contributing observations. This is useful because it enables an intuitive exploration of the contribution of variables to each principal component and the relationships between observations in the reduced-dimensional space.

```{r, echo=FALSE}
fviz_pca_biplot(pca_result, repel = TRUE,
                select.ind = list(contrib = 30), #Top 30 most contributing individuals
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969"  # Individuals color
                )
```



```{r}
# Percentage of variance explained by each principal component
var_explained <- pca_result$sdev^2 / sum(pca_result$sdev^2) * 100

# Cumulative percentage of variance explained
cumulative_var_explained <- cumsum(var_explained)

# Choose the number of components explaining 95% of cumulative variance
num_components <- which(cumulative_var_explained >= 95)[1]

# Retain the selected number of components
selected_components <- pca_result$x[, 1:num_components]

# Print the selected components
print(colnames(selected_components))
```



# References

```{=tex}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\vspace*{-0.2in}
```
\noindent
